{
    "questions":[
        {
            "question": "You are running a BigQuery project using the on-demand billing model, and you are executing a change data capture (CDC) process that ingests data. The CDC process loads 1 GB of data every 10 minutes into a temporary table and performs a merge into a 10 TB target table. This process is resource-intensive and involves significant scanning. You want to explore options to implement a predictable cost model. You need to create a BigQuery reservation based on usage data gathered from BigQuery Monitoring and apply the reservation to the CDC process. What should you do to create a BigQuery reservation that applies to the CDC process and ensures predictable costs?",
            "category": ["BigQuery"],
            "answers": [
                "Create a BigQuery reservation for the dataset.",
                "Create a BigQuery reservation for the job.",
                "Create a BigQuery reservation for the service account running the job.",
                "Create a BigQuery reservation for the project."
            ],
            "correct": 3,
            "explanation": "<strong>Answer: D - Create a BigQuery reservation for the project.</strong><br><br><strong>‚úÖ Why this is correct:</strong><br>BigQuery reservations are allocated at the <strong>project level</strong>. When you create a reservation, you're reserving compute capacity (slots) for all queries and loads running within that project. This makes costs more predictable.<br><br><strong>üéØ Applied to the CDC process:</strong><ul><li>Any job running within the project ‚Äî including the CDC process ‚Äî will automatically use the reserved slots.</li><li>There's no need to explicitly link the reservation to a dataset, job, or service account.</li></ul><br><strong>‚ùå Why other options are incorrect:</strong><ul><li><strong>A - Dataset:</strong> A reservation for a single dataset won't cover multiple datasets used by the CDC process (temporary + target tables).</li><li><strong>B - Job:</strong> Reserving capacity for a single job doesn't help when the process includes multiple steps.</li><li><strong>C - Service Account:</strong> Limited to the jobs run by that account. CDC may involve multiple service accounts or require broader coverage.</li></ul><br><strong>üìö Further Reading:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery-reservation/docs/overview\" target=\"_blank\">BigQuery Reservations Overview</a><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/pricing\" target=\"_blank\">BigQuery Pricing</a><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/monitoring\" target=\"_blank\">BigQuery Monitoring</a>",
            "verified": true
        },
        {
            "question": "A data science team at your company needs to perform exploratory analysis on a new, large dataset. Their query patterns will be unpredictable‚Äîsometimes running many small queries, and other times running large, complex queries. They will not be using the environment every day. Which billing model is most cost-effective for this team's initial work?",
            "category": ["BigQuery"],
            "answers": [
                "The Capacity (Flat-Rate) model, because it provides performance guarantees.",
                "The On-Demand model, because they only pay for the specific queries they run.",
                "The Capacity (Flat-Rate) model, because costs are predictable.",
                "The On-Demand model, because it offers dedicated slot capacity."
            ],
            "correct": 1,
            "explanation": "<strong>Answer: B - The On-Demand model, because they only pay for the specific queries they run.</strong><br><br><strong>‚úÖ Why this is correct:</strong><br>The team‚Äôs query workload is <strong>unpredictable</strong> and <strong>intermittent</strong>. With the On-Demand model, you only pay for the queries that are actually executed. This avoids wasted spending during idle periods.<br><br><strong>‚ùå Why other options are incorrect:</strong><ul><li><strong>A & C - Capacity Model:</strong> Flat-rate billing is best for stable, high-volume workloads. In this case, it would result in paying for unused capacity.</li><li><strong>D - On-Demand doesn‚Äôt provide dedicated slot capacity:</strong> That‚Äôs a feature of the Capacity model, not On-Demand.</li></ul><br><strong>üìå Summary:</strong> For occasional and exploratory use, On-Demand offers the most cost-efficient and flexible billing.<br><br><strong>üìö Further Reading:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/pricing\" target=\"_blank\">BigQuery Pricing Models</a>",
            "ai_generated": true,
            "verified": true
        },
        {
            "question": "You are using the on-demand billing model. You run the following query on a 5 TB table named customer_events, where the event_details column is 2 TB in size and the user_id column is 50 GB in size.<pre><code>SELECT user_id<br>FROM<br>`my_project.my_dataset.customer_events`<br>WHERE event_type = 'purchase'</code></pre>What is the primary factor that will determine the cost of this query?",
            "category": ["BigQuery"],
            "answers": [
                "The number of rows returned by the query.",
                "The total size of the customer_events table (5 TB).",
                "The amount of processing power (slots) used to execute the query.",
                "The total size of the columns scanned by the query (user_id and event_type)."
            ],
            "correct": 3,
            "explanation": "<strong>Answer: D - The total size of the columns scanned by the query (user_id and event_type).</strong><br><br><strong>‚úÖ Why this is correct:</strong><br>BigQuery uses a <strong>columnar storage format</strong>, which means it only reads and charges for the columns involved in the query.<br><br>In this case, only the <code>user_id</code> and <code>event_type</code> columns are referenced ‚Äî the <code>event_details</code> column and the rest of the 5 TB table are ignored. Therefore, cost is based on the size of just those two columns.<br><br><strong>‚ùå Why other options are incorrect:</strong><ul><li><strong>A - Number of rows returned:</strong> BigQuery charges by data processed, not output size.</li><li><strong>B - Total table size:</strong> Irrelevant unless all columns are scanned.</li><li><strong>C - Processing power:</strong> Affects performance, not billing in the on-demand model.</li></ul><br><strong>üìö Further Reading:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/pricing\" target=\"_blank\">BigQuery Pricing</a><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax\" target=\"_blank\">BigQuery Query Reference</a>",
            "ai_generated": true,
            "verified": true
        },
        {
            "question": "Your company's finance department requires a fixed, budgetable monthly cost for your main data warehousing project. The project runs predictable ETL jobs every hour. Which billing model meets this requirement and why?",
            "category": ["BigQuery"],
            "answers": [
                "The On-Demand model, because the cost scales directly with usage.",
                "The Capacity (Flat-Rate) model, because you purchase a fixed amount of processing capacity for a set price.",
                "The On-Demand model, because it requires no upfront commitment.",
                "The Capacity (Flat-Rate) model, because it charges based on the amount of data scanned."
            ],
            "correct": 1,
            "explanation": "<strong>Answer: B - The Capacity (Flat-Rate) model, because you purchase a fixed amount of processing capacity for a set price.</strong><br><br><strong>‚úÖ Why this is correct:</strong><br>The finance department requires <strong>predictable, fixed monthly billing</strong>. The Capacity model meets this requirement by allowing you to reserve a specific number of slots at a <strong>fixed monthly price</strong>, regardless of how much data you scan.<br><br><strong>‚ùå Why other options are incorrect:</strong><ul><li><strong>A - On-Demand:</strong> Costs vary based on usage, which makes budgeting difficult.</li><li><strong>C - On-Demand:</strong> No upfront commitment, but no cost predictability either.</li><li><strong>D - Flat-Rate:</strong> Incorrect reasoning ‚Äî flat-rate does <strong>not</strong> charge based on scanned data; that's On-Demand.</li></ul><br><strong>üìå Summary:</strong> For fixed budgeting and predictable workloads (like hourly ETL), the Flat-Rate model is the best fit.<br><br><strong>üìö Further Reading:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/reservations\" target=\"_blank\">BigQuery Reservations</a><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/pricing\" target=\"_blank\">BigQuery Pricing</a>",
            "ai_generated": true,
            "verified": true
        },
        {
            "question": "A retail company wants to synchronize its on-premise transactional database with BigQuery in near real-time. The primary goal is to minimize the performance impact on their production database, which is constantly handling customer orders. Which CDC method is most suitable for this requirement?",
            "category": ["Change Data Capture", "CDC", "BigQuery"],
            "answers": [
                "Query-Based CDC, because it is the simplest to implement.",
                "Log-Based CDC, because it reads from the database transaction log without adding load to the tables.",
                "Trigger-Based CDC, because it provides a reliable audit trail of all changes.",
                "Full Table Replication, because it ensures data consistency by copying the entire table every time."
            ],
            "correct": 1,
            "explanation": "<strong>Answer: B - Log-Based CDC, because it reads from the database transaction log without adding load to the tables.</strong><br><br><strong>‚úÖ Why this is correct:</strong><br>Log-based CDC works by reading the <strong>database's transaction log</strong>, allowing changes to be captured without querying the live tables. This minimizes impact on performance and is ideal for high-throughput production environments.<br><br><strong>‚ùå Why other options are incorrect:</strong><ul><li><strong>A - Query-Based CDC:</strong> Involves scanning tables frequently, adding read load and risking performance degradation.</li><li><strong>C - Trigger-Based CDC:</strong> Adds overhead on every write operation (INSERT, UPDATE, DELETE), slowing down transactional performance.</li><li><strong>D - Full Table Replication:</strong> Extremely resource-intensive and network-heavy. Re-copying large tables frequently is inefficient and not suitable for near real-time sync.</li></ul><br><strong>üìå Summary:</strong> For near real-time data sync with minimal impact on transactional systems, log-based CDC is the best fit.<br><br><strong>üìö Further Reading:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/architecture/streaming-change-data-capture\" target=\"_blank\">Streaming Change Data Capture to BigQuery</a><br>‚Ä¢ <a href=\"https://en.wikipedia.org/wiki/Change_data_capture\" target=\"_blank\">Change Data Capture (Wikipedia)</a>",
            "ai_generated": true,
            "verified": true
        },
        {
            "question": "You have been tasked with setting up a CDC pipeline from a PostgreSQL database running on-premise to a BigQuery data warehouse. The requirements state that the solution must be serverless, fully managed, and require minimal custom code to implement the MERGE logic in BigQuery. Which Google Cloud service is designed specifically for this use case?",
            "category": ["Change Data Capture", "CDC", "BigQuery", "Datastream", "Dataflow"],
            "answers": [
                "Dataflow, because it offers maximum flexibility for data transformations.",
                "Cloud Functions, because it can be triggered to run SQL commands.",
                "Datastream, because it is a managed, serverless CDC service that integrates directly with BigQuery.",
                "BigQuery Data Transfer Service, because it is used for loading data from SaaS applications."
            ],
            "correct": 2,
            "explanation": "<strong>Answer: C - Datastream, because it is a managed, serverless CDC service that integrates directly with BigQuery.</strong><br><br><strong>‚úÖ Why this is correct:</strong><br>Datastream is designed for <strong>change data capture</strong> from sources like PostgreSQL into BigQuery using a <strong>serverless</strong>, <strong>fully managed</strong> pipeline. It minimizes custom code and automates integration, including support for writing into BigQuery with merge semantics via Dataflow templates.<br><br><strong>‚ùå Why other options are incorrect:</strong><ul><li><strong>A - Dataflow:</strong> Although flexible, it requires writing the CDC and merge logic manually, which violates the requirement for minimal custom code.</li><li><strong>B - Cloud Functions:</strong> You'd have to build everything manually (triggers, merge logic, state management), which is neither scalable nor managed for CDC.</li><li><strong>D - BigQuery Data Transfer Service:</strong> This tool is for scheduled imports from SaaS platforms, not relational databases or real-time CDC pipelines.</li></ul><br><strong>üìå Summary:</strong> For minimal effort and fully managed CDC from PostgreSQL to BigQuery, Datastream is the best choice.<br><br><strong>üìö Further Reading:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/datastream/docs/overview\" target=\"_blank\">Datastream Overview</a><br>‚Ä¢ <a href=\"https://cloud.google.com/architecture/streaming-change-data-capture\" target=\"_blank\">Streaming CDC to BigQuery</a>",
            "ai_generated": true,
            "verified": true
        },
        {
            "question": "A team implements a simple CDC process for their products table using a last_updated_timestamp column. The process runs every 10 minutes and copies any rows with a newer timestamp to BigQuery. They soon discover that when products are discontinued and their rows are deleted from the source table, the old product information remains in BigQuery indefinitely. Why is this happening?",
            "category": ["Change Data Capture", "CDC", "BigQuery"],
            "answers": [
                "The MERGE statement in BigQuery is not configured correctly.",
                "The CDC process has insufficient permissions to read the source table.",
                "The timestamp-based CDC method cannot capture DELETE operations, as the deleted rows are no longer there to be queried.",
                "There is a network issue preventing the changes from reaching BigQuery."
            ],
            "correct": 2,
            "explanation": "<strong>Answer: C - The timestamp-based CDC method cannot capture DELETE operations, as the deleted rows are no longer there to be queried.</strong><br><br><strong>‚úÖ Why this is correct:</strong><br>When CDC is implemented using a <code>last_updated_timestamp</code>, it only captures rows that were inserted or updated ‚Äî because those are the only rows still available for querying. <strong>Deleted rows no longer exist</strong> in the source database, so they‚Äôre invisible to this approach.<br><br><strong>‚ùå Why other options are incorrect:</strong><ul><li><strong>A - Incorrect MERGE statement:</strong> Even a perfect MERGE can‚Äôt delete rows that the CDC process never captures.</li><li><strong>B - Insufficient permissions:</strong> Would result in failure to read any rows, not just deletes. Updates and inserts would also fail.</li><li><strong>D - Network issue:</strong> Would affect all change types (INSERT/UPDATE/DELETE), not just deletions. The problem here is systematic, not connectivity-based.</li></ul><br><strong>üìå Summary:</strong> Timestamp-based CDC is simple but blind to DELETE operations. To capture deletions, a <strong>log-based</strong> CDC approach is required.<br><br><strong>üìö Further Reading:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/architecture/streaming-change-data-capture\" target=\"_blank\">Google Cloud CDC Best Practices</a><br>‚Ä¢ <a href=\"https://en.wikipedia.org/wiki/Change_data_capture\" target=\"_blank\">Change Data Capture (Wikipedia)</a>",
            "ai_generated": true,
            "verified": true
        },
        {
            "question": "Your company is migrating from on-premises data warehousing to BigQuery. The current solution uses trigger-based CDC with daily updates. The goal is to improve near real-time CDC using log-based streaming, with minimal latency and reduced compute overhead when applying changes to reporting tables. Which two steps should they take to ensure that changes are available in the BigQuery reporting table with minimal latency while reducing compute overhead? (Choose two.)",
            "category": ["Change Data Capture", "CDC", "BigQuery"],
            "answers": [
                "Perform a DML INSERT, UPDATE, or DELETE to replicate each individual CDC record in real time directly on the reporting table.",
                "Insert each new CDC record and corresponding operation type to a staging table in real time.",
                "Periodically DELETE outdated records from the reporting table.",
                "Periodically use a DML MERGE to perform several DML INSERT, UPDATE, and DELETE operations at the same time on the reporting table.",
                "Insert each new CDC record and corresponding operation type in real time to the reporting table, and use a materialized view to expose only the newest version of each unique record."
            ],
            "correct": [1, 3],
            "explanation": "<strong>Answers: B and D</strong><br><br><strong>‚úÖ B - Staging Table for Real-Time Ingestion:</strong><br>Inserting CDC records into a <strong>staging table</strong> allows for low-latency data capture without overwhelming the reporting table with frequent writes. It decouples ingestion from transformation and supports buffering and recovery.<br><br><strong>‚úÖ D - MERGE for Efficient Batch Updates:</strong><br>Using <code>MERGE</code> to apply multiple INSERT, UPDATE, and DELETE operations in one batch is <strong>compute-efficient</strong> and ensures data consistency. It reduces write overhead and supports transactional semantics.<br><br><strong>‚ùå Why other options are incorrect:</strong><ul><li><strong>A:</strong> Real-time DML on the reporting table is <strong>inefficient and high-overhead</strong>, especially at scale.</li><li><strong>C:</strong> Deleting outdated records may be part of housekeeping but is not a core CDC performance strategy.</li><li><strong>E:</strong> Writing directly to the reporting table in real time still causes performance issues. Materialized views help read performance, not write optimization.</li></ul><br><strong>üìå Summary:</strong> A robust CDC architecture for BigQuery should combine <strong>real-time staging</strong> with <strong>periodic batch merges</strong> to optimize both freshness and performance.<br><br><strong>üìö Further Reading:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#merge_statement\" target=\"_blank\">BigQuery MERGE Statement</a><br>‚Ä¢ <a href=\"https://cloud.google.com/architecture/real-time-data-analysis-with-change-data-capture\" target=\"_blank\">CDC Best Practices on Google Cloud</a><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/docs/data-manipulation-language\" target=\"_blank\">BigQuery DML Documentation</a>",
            "verified": true
        },
        {
            "question": "You are migrating your on-premises data warehouse to BigQuery. One of the upstream data sources is a MySQL database hosted on-premises without any public IP addresses. You need to securely ingest data into BigQuery, ensuring the transfer does not traverse the public internet. What should you do?",
            "category": ["Change Data Capture", "CDC", "BigQuery", "Datastream"],
            "answers": [
                "Update your existing on-premises ETL tool to write to BigQuery by using the BigQuery ODBC driver. Set up the proxy parameter in the simba.googlebigqueryodbc.ini file to point to your data center‚Äôs NAT gateway.",
                "Use Datastream to replicate data from your on-premises MySQL database to BigQuery. Set up Cloud Interconnect between your on-premises data center and Google Cloud. Use Private connectivity as the connectivity method and allocate an IP address range within your VPC network to the Datastream connectivity configuration. Use Server-only as the encryption type when setting up the connection profile in Datastream.",
                "Use Datastream to replicate data from your on-premises MySQL database to BigQuery. Use Forward-SSH tunnel as the connectivity method to establish a secure tunnel between Datastream and your on-premises MySQL database through a tunnel server in your on-premises data center. Use None as the encryption type when setting up the connection profile in Datastream.",
                "Use Datastream to replicate data from your on-premises MySQL database to BigQuery. Gather Datastream public IP addresses of the Google Cloud region that will be used to set up the stream. Add those IP addresses to the firewall allowlist of your on-premises data center. Use IP Allowlisting as the connectivity method and Server-only as the encryption type when setting up the connection profile in Datastream."
            ],
            "correct": 1,
            "explanation": "<strong>Answer: B - Use Datastream with Cloud Interconnect and Private Connectivity.</strong><br><br><strong>‚úÖ Why this is correct:</strong><ul><li><strong>Cloud Interconnect</strong> provides a dedicated, private network path between your on-premises data center and Google Cloud, completely bypassing the public internet.</li><li><strong>Datastream‚Äôs Private Connectivity</strong> lets you replicate from a MySQL source over this secure private path using internal IP ranges in your VPC.</li><li><strong>Server-only encryption</strong> ensures traffic is encrypted in-transit, even over private networks, by enforcing TLS/SSL from Datastream to the MySQL server.</li></ul><br><strong>‚ùå Why the other options are incorrect:</strong><ul><li><strong>A - ODBC via NAT:</strong> This routes traffic through the public internet via NAT and violates the ‚Äúno public internet‚Äù constraint.</li><li><strong>C - SSH Tunnel with no encryption:</strong> Although SSH tunnels provide private routing, setting <code>encryption=None</code> poses a serious security risk. All data would move unencrypted within the tunnel.</li><li><strong>D - IP Allowlisting:</strong> Requires exposing your database to Datastream's <em>public IPs</em>. This goes against both the <strong>no public IP</strong> and <strong>no public internet</strong> requirements.</li></ul><br><strong>üìå Summary:</strong> Using <strong>Datastream + Cloud Interconnect + Private Connectivity + Server-only Encryption</strong> delivers a fully private, secure, and efficient data replication pipeline.<br><br><strong>üìö References:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/datastream/docs\" target=\"_blank\">Datastream Documentation</a><br>‚Ä¢ <a href=\"https://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview\" target=\"_blank\">Cloud Interconnect Overview</a><br>‚Ä¢ <a href=\"https://cloud.google.com/datastream/docs/concepts/connectivity/private-connectivity\" target=\"_blank\">Datastream Private Connectivity</a>",
            "verified": true
        },
        {
            "question": "You have an Oracle database deployed in a VM as part of a Virtual Private Cloud (VPC) network. You want to replicate and continuously synchronize 50 tables to BigQuery. You want to minimize the need to manage infrastructure. What should you do?",
            "category": ["Change Data Capture", "CDC", "BigQuery", "Datastream"],
            "answers": [
                "Deploy Apache Kafka in the same VPC network, use Kafka Connect Oracle Change Data Capture (CDC), and Dataflow to stream the Kafka topic to BigQuery.",
                "Create a Pub/Sub subscription to write to BigQuery directly. Deploy the Debezium Oracle connector to capture changes in the Oracle database, and sink to the Pub/Sub topic.",
                "Deploy Apache Kafka in the same VPC network, use Kafka Connect Oracle change data capture (CDC), and the Kafka Connect Google BigQuery Sink Connector.",
                "Create a Datastream service from Oracle to BigQuery, use a private connectivity configuration to the same VPC network, and a connection profile to BigQuery."
            ],
            "correct": 3,
            "explanation": "<strong>Answer: D - Use Datastream with private connectivity and connection profiles.</strong><br><br><strong>‚úÖ Why this is correct:</strong><br>Datastream is a <strong>serverless, fully managed CDC service</strong> optimized for sources like Oracle and targets like BigQuery. It eliminates the operational overhead of managing third-party systems like Kafka or Debezium.<br>‚Ä¢ Private connectivity ensures secure, low-latency communication within your VPC.<br>‚Ä¢ Connection profiles allow for clean and reusable integration configs.<br>‚Ä¢ It supports <strong>continuous synchronization</strong> with minimal code or orchestration.<br><br><strong>‚ùå Why other options are incorrect:</strong><ul><li><strong>A:</strong> Kafka + Dataflow requires you to deploy and manage Kafka clusters and processing pipelines ‚Äî violating the requirement to minimize infrastructure management.</li><li><strong>B:</strong> Debezium + Pub/Sub is powerful, but still requires substantial setup and monitoring. Pub/Sub also isn't designed for direct Oracle CDC.</li><li><strong>C:</strong> Kafka Connect with BigQuery Sink is not serverless and shifts operational complexity to your team.</li></ul><br><strong>üìå Summary:</strong> If you want <strong>low-maintenance CDC replication from Oracle to BigQuery</strong>, Datastream with private connectivity is the most efficient, secure, and managed approach.<br><br><strong>üìö Further Reading:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/datastream/docs/overview\" target=\"_blank\">Datastream Overview</a><br>‚Ä¢ <a href=\"https://cloud.google.com/datastream/docs/sources/oracle\" target=\"_blank\">Datastream for Oracle</a><br>‚Ä¢ <a href=\"https://cloud.google.com/datastream/docs/targets/bigquery\" target=\"_blank\">Datastream for BigQuery</a><br>‚Ä¢ <a href=\"https://cloud.google.com/datastream/docs/concepts/connectivity/private-connectivity\" target=\"_blank\">Private Connectivity with Datastream</a>",
            "verified": true
        },
        {
            "question": "You are running your BigQuery project using the on-demand billing model and executing a change data capture (CDC) process that loads 1 GB of data every 10 minutes into a temporary table, then performs a MERGE into a 10 TB target table. This process is scan-intensive, and you want to switch to a predictable cost model. You gather utilization data from BigQuery Monitoring. What should you do?",
            "category": ["Change Data Capture", "CDC", "BigQuery", "Datastream"],
            "answers": [
                "Create a BigQuery reservation for the dataset.",
                "Create a BigQuery reservation for the job.",
                "Create a BigQuery reservation for the service account running the job.",
                "Create a BigQuery reservation for the project."
            ],
            "correct": 3,
            "explanation": "<strong>Answer: D - Create a BigQuery reservation for the project.</strong><br><br><strong>‚úÖ Why this is correct:</strong><br>BigQuery reservations allocate compute capacity (<strong>slots</strong>) to your project, allowing you to replace unpredictable on-demand billing with a flat-rate, predictable cost.<br><br>‚Ä¢ <strong>Project-level reservations</strong> automatically apply to all jobs run within the project, including your CDC workloads.<br>‚Ä¢ This ensures that scan-intensive MERGE operations can execute using the reserved slots, avoiding high and variable per-query costs.<br>‚Ä¢ BigQuery Monitoring helps you estimate slot usage and right-size the reservation.<br><br><strong>‚ùå Why the other options are incorrect:</strong><ul><li><strong>A - Dataset:</strong> Reservations are not applied at the dataset level.</li><li><strong>B - Job:</strong> You don‚Äôt create a reservation for individual jobs; the reservation applies at a broader level.</li><li><strong>C - Service account:</strong> While job assignments can be influenced by service accounts, the reservation itself is still scoped at the project level.</li></ul><br><strong>üìå Summary:</strong> Use <code>project-level reservations</code> with BigQuery Monitoring insights to control costs for predictable, high-volume workloads like CDC.<br><br><strong>üìö Further Reading:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/docs/reservations-intro\" target=\"_blank\">BigQuery Reservations Overview</a><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/docs/reservations-workload\" target=\"_blank\">Working with Reservations</a><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/docs/monitoring\" target=\"_blank\">Monitoring BigQuery Slots</a>",
            "verified": true
        },
        {
            "question": "You have terabytes of customer behavioral data streaming from Google Analytics into BigQuery daily. Your customers profile and preference data reside in two separate databases: a Cloud SQL for MySQL instance,  Cloud SQL for PostgreSQL instance The marketing team wants to run over 100 queries daily (up to 300 during sales) combining all these sources to target yearly active customers for campaigns. What should you do to minimize the load on Cloud SQL while supporting the query demand?",
            "category": ["Change Data Capture", "CDC", "BigQuery", "Datastream"],
            "answers": [
                "Create BigQuery connections to both Cloud SQL databases. Use BigQuery federated queries on the two databases and the Google Analytics data on BigQuery to run these queries.",
                "Create a job on Apache Spark with Dataproc Serverless to query both Cloud SQL databases and the Google Analytics data on BigQuery for these queries.",
                "Create streams in Datastream to replicate the required tables from both Cloud SQL databases to BigQuery for these queries.",
                "Create a Dataproc cluster with Trino to establish connections to both Cloud SQL databases and BigQuery, to execute the queries."
            ],
            "correct": 2,
            "explanation": "<strong>Answer: C - Create streams in Datastream to replicate the required tables from both Cloud SQL databases to BigQuery.</strong><br><br><strong>‚úÖ Why this is correct:</strong><ul><li><strong>Protects Cloud SQL:</strong> Avoids high-frequency direct querying that would strain the Cloud SQL production databases.</li><li><strong>Scalable and Real-time:</strong> <a href='https://cloud.google.com/datastream' target='_blank'>Datastream</a> uses <strong>CDC</strong> (Change Data Capture) to stream updates in near real-time to BigQuery, keeping data current and enabling responsive campaigns.</li><li><strong>Fast Queries:</strong> BigQuery is optimized for analytic workloads, offering high performance for complex joins and aggregations at scale.</li></ul><br><strong>‚ùå Why the other options are incorrect:</strong><ul><li><strong>A - Federated Queries:</strong> Still place live query load on Cloud SQL; not optimal for high query frequency.</li><li><strong>B - Dataproc with Spark:</strong> Adds architectural and operational complexity with no significant benefit for this use case.</li><li><strong>D - Dataproc with Trino:</strong> Overhead of managing a cluster and federated setup is unnecessary when Datastream offers a managed path.</li></ul><br><strong>üìå Summary:</strong> <strong>Datastream ‚Üí BigQuery</strong> is the cleanest, lowest-maintenance, and most performant solution for joining behavioral and profile data across systems at scale.<br><br><strong>üìö References:</strong><br>‚Ä¢ <a href='https://cloud.google.com/datastream/docs' target='_blank'>Datastream Documentation</a><br>‚Ä¢ <a href='https://cloud.google.com/bigquery/docs' target='_blank'>BigQuery Documentation</a><br>‚Ä¢ <a href='https://en.wikipedia.org/wiki/Change_data_capture' target='_blank'>Change Data Capture (CDC)</a>",
            "verified": true
        },
        {
            "question": "Which Google Cloud product is best suited for storing unstructured data like images and videos?",
            "category": ["Cloud Storage"],
            "answers": [
                "Cloud Storage",
                "Cloud SQL",
                "Bigtable",
                "BigQuery"
            ],
            "correct": 0,
            "explanation": "<strong>Answer: A - Cloud Storage</strong><br><br><strong>‚úÖ Why this is correct:</strong><ul><li><strong>Cloud Storage</strong> is explicitly designed for storing <em>unstructured data</em> such as images, videos, documents, and backups.</li><li>It is a scalable, durable, and cost-effective object storage system ideal for binary data and large file blobs.</li></ul><br><strong>‚ùå Why the other options are incorrect:</strong><ul><li><strong>Cloud SQL:</strong> A managed relational database for structured transactional workloads (e.g., MySQL, PostgreSQL, SQL Server). Not suitable for storing media files.</li><li><strong>Bigtable:</strong> A high-throughput NoSQL database built for time-series and analytical workloads. It‚Äôs used for structured, wide-column data, not unstructured content.</li><li><strong>BigQuery:</strong> A data warehouse optimized for querying structured and semi-structured datasets. It is not designed for storing binary files or media assets.</li></ul><br><strong>üìö Reference:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/storage\" target=\"_blank\">Google Cloud Storage Overview</a><br>‚Ä¢ <a href=\"https://cloud.google.com/docs/overview\" target=\"_blank\">Google Cloud Product Overview</a>",
            "verified": true
        },
        {
            "question": "What is the purpose of Analytics Hub in Google Cloud?",
            "category": "Google Cloud Platform - Data Sharing",
            "answers": [
                "To perform complex machine learning tasks on large datasets.",
                "To automate the process of data cleaning and transformation.",
                "To provide a centralized platform for data visualization.",
                "To enable secure and controlled data sharing within and outside an organization."
            ],
            "correct": 3,
            "explanation": "<strong>Answer: D - To enable secure and controlled data sharing within and outside an organization.</strong><br><br><strong>‚úÖ Why this is correct:</strong><ul><li>Analytics Hub allows organizations to <em>securely publish and subscribe</em> to BigQuery datasets.</li><li>It enables <em>in-place data sharing</em> (no need to copy datasets), giving providers fine-grained control over data usage and access.</li><li>This model promotes reuse, governance, and reduces storage redundancy.</li></ul><br><strong>‚ùå Why the other options are incorrect:</strong><ul><li><strong>A - Machine learning tasks:</strong> This is the domain of BigQuery ML or Vertex AI, not Analytics Hub.</li><li><strong>B - Data cleaning and transformation:</strong> That is best handled by Dataflow, Dataproc, or Dataform, which are designed for ETL/ELT operations.</li><li><strong>C - Data visualization:</strong> Analytics Hub does not provide visualization capabilities; tools like Looker Studio, Tableau, or Power BI integrate with BigQuery for that purpose.</li></ul><br><strong>üìö Reference:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/bigquery/docs/analytics-hub-introduction\" target=\"_blank\">Google Cloud Analytics Hub Overview</a>",
            "verified": true
        },
        {
            "question": "What is the primary function of a data engineer?",
            "category": ["Data Engineering"],
            "answers": [
                "Managing network security for data systems.",
                "Conducting statistical analysis on data.",
                "Building and maintaining data pipelines.",
                "Designing user interfaces for data visualization."
            ],
            "correct": 2,
            "explanation": "<strong>Answer: C - Building and maintaining data pipelines</strong><br><br><strong>‚úÖ Why this is correct:</strong><ul><li>Data engineers are responsible for designing, building, and maintaining the infrastructure that allows for efficient data collection, transformation, and storage.</li><li>They enable downstream use cases such as analytics, reporting, and machine learning by ensuring clean, reliable, and timely data flow.</li></ul><br><strong>‚ùå Why the other options are incorrect:</strong><ul><li><strong>A - Managing network security:</strong> This is the role of network or security engineers. Data engineers focus on the data layer, not network infrastructure.</li><li><strong>B - Conducting statistical analysis:</strong> This is a task for data analysts or data scientists who consume the data prepared by data engineers.</li><li><strong>D - Designing user interfaces:</strong> This falls under the domain of UI/UX designers and frontend developers, not data engineers.</li></ul><br><strong>üìö Reference:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/certification/data-engineer\" target=\"_blank\">Google Cloud Certified Professional Data Engineer</a>",
            "verified": true
        },
        {
            "question": "What is the key difference between a data lake and a data warehouse?",
            "category": ["Data Architecture", "Data lake", "Data warehouse"],
            "answers": [
                "Data lakes are used for real-time analytics, while data warehouses are used for long-term storage.",
                "Data lakes store only structured data, while data warehouses store unstructured data.",
                "Data lakes store raw, unprocessed data, while data warehouses store processed and organized data.",
                "Data lakes are managed by data scientists, while data warehouses are managed by data engineers."
            ],
            "correct": 2,
            "explanation": "<strong>Answer: C - Data lakes store raw, unprocessed data, while data warehouses store processed and organized data.</strong><br><br><strong>‚úÖ Why this is correct:</strong><ul><li>Data lakes serve as repositories for data in its original form‚Äîstructured, semi-structured, or unstructured.</li><li>They are designed to support a wide range of workloads, such as data science, machine learning, and exploratory analytics.</li><li>Data warehouses, in contrast, store data that has already been cleaned, transformed, and optimized for analytical queries and reporting.</li></ul><br><strong>‚ùå Why the other options are incorrect:</strong><ul><li><strong>A:</strong> Data lakes are not limited to real-time analytics and can be used for long-term data storage as well. Data warehouses are typically used for batch analytics and historical reporting.</li><li><strong>B:</strong> This is reversed. Data lakes store structured, semi-structured, and unstructured data. Data warehouses focus primarily on structured data.</li><li><strong>D:</strong> There is no strict role demarcation‚Äîdata engineers typically manage both environments, and data scientists consume data from both depending on their needs.</li></ul><br><strong>üìö Reference:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/architecture/data-lake-vs-data-warehouse\" target=\"_blank\">Google Cloud: Data Lake vs Data Warehouse</a>",
            "verified": true
        },
        {
            "question": "Which stage in a data pipeline involves modifying and preparing data for specific downstream requirements?",
            "category": ["Data Pipelines"],
            "answers": [
                "Replicate and migrate",
                "Store",
                "Ingest",
                "Transform"
            ],
            "correct": 3,
            "explanation": "<strong>Answer: D - Transform</strong><br><br><strong>‚úÖ Why this is correct:</strong><ul><li>The <strong>Transform</strong> stage is where data is adjusted, joined, filtered, or otherwise modified to meet the needs of downstream systems such as reporting tools, dashboards, or machine learning models.</li><li>This step tailors the raw or ingested data into a usable and meaningful format.</li></ul><br><strong>‚ùå Why the other options are incorrect:</strong><ul><li><strong>A - Replicate and migrate:</strong> Focuses on copying and moving data into Google Cloud, not preparing it for analysis.</li><li><strong>B - Store:</strong> Refers to where the data lands after it's been transformed or ingested, not the act of transformation itself.</li><li><strong>C - Ingest:</strong> Simply brings raw data into the pipeline; it does not alter or process it.</li></ul><br><strong>üìö Reference:</strong><br>‚Ä¢ <a href=\"https://cloud.google.com/architecture/data-pipeline-design-patterns\" target=\"_blank\">Data Pipeline Design Patterns ‚Äì Google Cloud</a>",
            "verified": true
        },
        {
            "question": "Which of the following services efficiently moves large datasets from on-premises, multicloud file systems, object stores, and HDFS into Cloud Storage and supports scheduled transfers?",
            "answers": [
            "Storage Transfer Service",
            "Vertex AI",
            "gcloud storage",
            "Transfer Appliance"
            ],
            "correct": 0,
            "explanation": "Storage Transfer Service is designed for large-scale, scheduled migrations from on-premises and cloud sources. It's a managed service optimized for automated transfers. Vertex AI is for machine learning, not data transfer. gcloud storage is a CLI for manual transfers. Transfer Appliance is a hardware device for offline migrations, not for scheduled cloud-based transfers.",
            "verified": true
        },
        {
            "question": "In Datastream event messages, which section contains the actual data changes in a key-value format?",
            "answers": [
            "Change log",
            "Payload",
            "Source-specific metadata",
            "Generic metadata"
            ],
            "correct": 1,
            "explanation": "The payload section in Datastream event messages holds the actual data changes in a key-value format. 'Change log' is a general term. Source-specific metadata holds origin-related metadata. Generic metadata holds context info like operation type.",
            "verified": true
        },
        {
            "question": "Which of the following tools is best suited for migrating very large datasets offline?",
            "answers": [
            "gcloud storage command",
            "Transfer Appliance",
            "Datastream",
            "Storage Transfer Service"
            ],
            "correct": 1,
            "explanation": "Transfer Appliance is a physical device designed for securely transferring very large datasets offline to Google Cloud. The other options are for online or streaming transfers.",
            "verified": true
        },
        {
            "question": "The ease of migrating data is heavily influenced by which two factors?",
            "answers": [
            "Data size and network bandwidth",
            "Data source and destination platform",
            "Data complexity and network latency",
            "Data format and storage type"
            ],
            "correct": 0,
            "explanation": "Data size and network bandwidth are the two primary factors that impact the speed and difficulty of data migration. While tools and formats matter, physical limits like size and throughput dominate migration feasibility.",
            "verified": true
        },
        {
            "question": "Which tool or service uses the ‚Äúcp‚Äù command to facilitate ad-hoc transfers directly to Cloud Storage from various on-premise sources?",
            "answers": [
            "Storage Transfer Service",
            "gcloud storage command",
            "Transfer Appliance",
            "Datastream"
            ],
            "correct": 1,
            "explanation": "The gcloud storage CLI supports ad-hoc file transfers using commands like 'gcloud storage cp'. It is suitable for quick, manual tasks. The others are managed services or hardware-based tools not intended for ad-hoc CLI-based transfers.",
            "verified": true
        }
    ]
}